---
title: 'Data Science Capstone: Exploratory Analysis'
author: "Jose M. Perez-Sanchez"
date: "December 20, 2015"
output: html_document
---

```{r, echo=FALSE}
library(knitr)
library(ggplot2)
require(gridExtra)
source("config.vars.R")
source("explore.R")
```

# Introduction

This report contains an exploratory analysis of the dataset provided for the capstone project of the JHU Data Science Coursera Specialization. The dataset is a text corpus containing documents in English, German, Russian and Finish. We analyze in this report the American English portion of the corpus which is contained in the three plain text files:

* en_US.blogs.txt
* en_US.news.txt
* en_US.twitter.txt

The dataset was downloaded from a Coursera provided [link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip) and according to the course documentation it was created from a corpus called HC Corpora. More info about how the original corpus was compiled can be found in the [About the Corpora](http://www.corpora.heliohost.org/aboutcorpus.html) section of the HC Corpora website. 

# Data loading

The documents were loaded into R using the *readLines* function with a "unknown" value for the text encoding. The document contained "bad unicode character sequences", typical from the document being processed at some point in its life with the wrong encoding. Those occurrences were fixed using the following substitutions:


```{r, echo=FALSE}
#unicode.fixes <- data.frame("Bad Unicode sequences"=c('Ã¢â¬Â¦', 'Ã¢â¬â', 'Ã¢â¬â', 'Ã¢â¬â¢', 'Ã¢â¬Å', 'Ã¢â¬[[:cntrl:]]'), "Replacement"= c('â¦', 'â', 'â', 'â', 'â', 'â'))
#kable(unicode.fixes)
```

# Exploratory analysys

## File character composition

```{r, echo=FALSE}
data.raw <- load.corpus.content(corpus.dir, max.lines=line.limit, encoding=text.encoding)
file.composition <- get.pattern.stats(data.raw)
kable(file.composition[, c("file", "alphanumeric", "space", "digit", "punctuation", "other", "nchar", "lines")], digits=4, col.names=c("File", "Alphanumeric", "Space", "Digit", "Punctuation", "Other", "Number of characters", "Number of lines"), caption="Character composition of the corpus files. The columns Alphanumeric, Space, Digit and Punctuation show the fraction of characters in each class. The column Other shows the fraction of characters that do not belong in any of the listed categories.")
```

# N-Gram statistics

## Unigrams (words)

```{r, echo=FALSE}
content <- fix.unicode(do.call(paste, as.list(data.raw$content)))
corpus <- create.corpus(content)

dtm.1 <- DocumentTermMatrix(corpus, control=list(wordLengths=c(min.word.len, max.word.len)))
ngram.1.freq <- get.ngram.freq(dtm.1)

dtm.2 <- DocumentTermMatrix(corpus, control = list(tokenize = NGramTokenizerFuncBuilder(2)))
ngram.2.freq <- get.ngram.freq(dtm.2)

dtm.3 <- DocumentTermMatrix(corpus, control = list(tokenize = NGramTokenizerFuncBuilder(3)))
ngram.3.freq <- get.ngram.freq(dtm.3)
```

In summary:

* The corpus contains `r nrow(ngram.1.freq)` unigrams (words)
* `r 100 * sum(ngram.1.freq$freq[ngram.1.freq$stopwords > 0])` percent of the occurrences are stop words.
* The `r sum(cumsum(ngram.1.freq$freq) < 0.5)` most frequent words account for approximately fifty percent of the word occurrences.
* The `r sum(cumsum(ngram.1.freq$freq) < 0.9)` most frequent words account for approximately ninety percent of the word occurrences.

```{r, echo=FALSE}
plot.num.words = 25
plot.colors = ggplotColours(2)
plot.sw0 <- ggplot(data=subset(ngram.1.freq, stopwords==0)[1:plot.num.words, ], aes(x=reorder(ngram, freq), y=freq)) + geom_bar(stat='identity', fill=plot.colors[2]) + labs(y="Probability", x="1-gram") + coord_flip() + ggtitle("Without stop words")
plot.sw1 <- ggplot(data=subset(ngram.1.freq, stopwords>0)[1:plot.num.words, ], aes(x=reorder(ngram, freq), y=freq)) + geom_bar(stat='identity', fill=plot.colors[1]) + labs(y="Probability", x="1-gram") + coord_flip() + ggtitle("With stop words")
grid.arrange(plot.sw1, plot.sw0, ncol=2)
```

## Bigrams

```{r, echo=FALSE}
plot.sw0 <- ggplot(data=subset(ngram.2.freq, stopwords==0)[1:plot.num.words, ], aes(x=reorder(ngram, freq), y=freq)) + geom_bar(stat='identity', fill=plot.colors[2]) + labs(y="Probability", x="2-gram") + coord_flip() + ggtitle("Without stop words")
plot.sw1 <- ggplot(data=subset(ngram.2.freq, stopwords>0)[1:plot.num.words, ], aes(x=reorder(ngram, freq), y=freq)) + geom_bar(stat='identity', fill=plot.colors[1]) + labs(y="Probability", x="2-gram") + coord_flip() + ggtitle("With stop words")
grid.arrange(plot.sw1, plot.sw0, ncol=2)
```

## Trigrams

```{r, echo=FALSE}
plot.sw0 <- ggplot(data=subset(ngram.3.freq, stopwords==0)[1:plot.num.words, ], aes(x=reorder(ngram, freq), y=freq)) + geom_bar(stat='identity', fill=plot.colors[2]) + labs(y="Probability", x="3-gram") + coord_flip() + ggtitle("Without stop words")
plot.sw1 <- ggplot(data=subset(ngram.3.freq, stopwords>0)[1:plot.num.words, ], aes(x=reorder(ngram, freq), y=freq)) + geom_bar(stat='identity', fill=plot.colors[1]) + labs(y="Probability", x="3-gram") + coord_flip() + ggtitle("With stop words")
grid.arrange(plot.sw1, plot.sw0, ncol=2)
```



```{r, echo=FALSE}
```